exp_name: puppersim  # the name of this experiment
seed: 1   # seed of the experiment 
torch_deterministic: True   # if toggled, `torch.backends.cudnn.deterministic=False` 
cuda: True   # if toggled, cuda will be enabled by default 
track: False   # if toggled, this experiment will be tracked with Weights and Biases 
wandb_project_name: "cleanRL"   # the wandb's project name 
wandb_entity: None   # the entity (team) of wandb's project
capture_video: False   # whether to capture videos of the agent performances (check out `videos` folder)
save_model: False   # whether to save model into the `runs/{run_name}` folder 

# Algorithm specific arguments
env_id: "Puppersim-v0"   # the id of the environment
total_timesteps: 10_000_000   # total timesteps of the experiments
learning_rate: 3e-4   # the learning rate of the optimizer 
num_envs: 32   # the number of parallel game environments 
num_steps: 2048   # the number of steps to run in each environment per policy rollout 
anneal_lr: True   # Toggle learning rate annealing for policy and value networks 
gamma: 0.99   # the discount factor gamma 
gae_lambda: 0.95   # the lambda for the general advantage estimation 
num_minibatches: 32   # the number of mini-batches 
update_epochs: 10   # the K epochs to update the policy 
norm_adv: True   # Toggles advantages normalization 
clip_coef: 0.2   # the surrogate clipping coefficient 
clip_vloss: True   # Toggles whether or not to use a clipped loss for the value function, as per the paper. 
ent_coef: 0.0   # coefficient of the entropy 
vf_coef: 0.5   # coefficient of the value function 
max_grad_norm: 0.5   # the maximum norm for the gradient clipping 
target_kl: None   # the target KL divergence threshold 

# to be filled in runtime
batch_size: 0   # the batch size (computed in runtime) 
minibatch_size: 0   # the mini-batch size (computed in runtime) 
num_iterations: 0   # the number of iterations (computed in runtime) 
